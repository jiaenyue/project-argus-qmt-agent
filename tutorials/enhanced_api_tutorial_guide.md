# å¢å¼ºå†å²Kçº¿APIä½¿ç”¨æŒ‡å—

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨å¢å¼ºçš„å†å²Kçº¿APIï¼ŒåŒ…æ‹¬å¤šå‘¨æœŸæ”¯æŒã€æ•°æ®è´¨é‡ç›‘æ§ã€ç¼“å­˜ä¼˜åŒ–å’Œæ€§èƒ½è°ƒä¼˜ã€‚

## ç›®å½•

1. [åŠŸèƒ½æ¦‚è¿°](#åŠŸèƒ½æ¦‚è¿°)
2. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
3. [å¤šå‘¨æœŸæ•°æ®è·å–](#å¤šå‘¨æœŸæ•°æ®è·å–)
4. [æ•°æ®è´¨é‡ç›‘æ§](#æ•°æ®è´¨é‡ç›‘æ§)
5. [ç¼“å­˜ä¼˜åŒ–ç­–ç•¥](#ç¼“å­˜ä¼˜åŒ–ç­–ç•¥)
6. [æ€§èƒ½ä¼˜åŒ–æŠ€å·§](#æ€§èƒ½ä¼˜åŒ–æŠ€å·§)
7. [é”™è¯¯å¤„ç†æœºåˆ¶](#é”™è¯¯å¤„ç†æœºåˆ¶)
8. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
9. [ç¤ºä¾‹ä»£ç ](#ç¤ºä¾‹ä»£ç )

## åŠŸèƒ½æ¦‚è¿°

å¢å¼ºçš„å†å²Kçº¿APIæä¾›ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½ï¼š

### ğŸš€ æ ¸å¿ƒç‰¹æ€§

- **å¤šå‘¨æœŸæ”¯æŒ**: æ”¯æŒ8ç§æ—¶é—´å‘¨æœŸï¼ˆ1m, 5m, 15m, 30m, 1h, 1d, 1w, 1Mï¼‰
- **æ•°æ®è´¨é‡ç›‘æ§**: å®æ—¶ç›‘æ§æ•°æ®å®Œæ•´æ€§ã€å‡†ç¡®æ€§å’Œä¸€è‡´æ€§
- **æ™ºèƒ½ç¼“å­˜**: æ ¹æ®æ•°æ®å‘¨æœŸè‡ªåŠ¨è®¾ç½®ç¼“å­˜TTLç­–ç•¥
- **æ ‡å‡†åŒ–æ ¼å¼**: ç»Ÿä¸€çš„JSONæ ¼å¼å’Œæ•°æ®ç±»å‹
- **å¼‚æ­¥å¤„ç†**: æ”¯æŒé«˜å¹¶å‘å¼‚æ­¥è¯·æ±‚
- **é”™è¯¯æ¢å¤**: å®Œå–„çš„é‡è¯•æœºåˆ¶å’Œé™çº§ç­–ç•¥

### ğŸ“Š è´¨é‡ä¿è¯

- **OHLCé€»è¾‘éªŒè¯**: è‡ªåŠ¨éªŒè¯å¼€é«˜ä½æ”¶ä»·æ ¼é€»è¾‘å…³ç³»
- **å¼‚å¸¸æ•°æ®æ£€æµ‹**: ä½¿ç”¨ç»Ÿè®¡æ–¹æ³•æ£€æµ‹ä»·æ ¼å’Œæˆäº¤é‡å¼‚å¸¸
- **æ•°æ®å®Œæ•´æ€§æ£€æŸ¥**: æ£€æµ‹ç¼ºå¤±å€¼å’Œæ•°æ®æ ¼å¼é”™è¯¯
- **è´¨é‡è¯„åˆ†ç³»ç»Ÿ**: æä¾›ç»¼åˆè´¨é‡è¯„åˆ†å’Œæ”¹è¿›å»ºè®®

### âš¡ æ€§èƒ½ä¼˜åŒ–

- **å·®å¼‚åŒ–ç¼“å­˜**: ä¸åŒå‘¨æœŸæ•°æ®ä½¿ç”¨ä¸åŒçš„ç¼“å­˜TTL
- **æ‰¹é‡è·å–**: æ”¯æŒå¹¶å‘è·å–å¤šåªè‚¡ç¥¨æ•°æ®
- **å†…å­˜ä¼˜åŒ–**: æ™ºèƒ½å†…å­˜ç®¡ç†å’Œåƒåœ¾å›æ”¶
- **æ€§èƒ½ç›‘æ§**: å®æ—¶ç›‘æ§å“åº”æ—¶é—´å’Œç¼“å­˜å‘½ä¸­ç‡

## å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
pip install asyncio pandas numpy
```

### åŸºç¡€ä½¿ç”¨

```python
import asyncio
from src.argus_mcp.api.enhanced_historical_api import (
    EnhancedHistoricalDataAPI,
    HistoricalDataRequest
)
from src.argus_mcp.data_models.historical_data import SupportedPeriod

async def basic_example():
    # åˆå§‹åŒ–API
    api = EnhancedHistoricalDataAPI()
    
    # åˆ›å»ºè¯·æ±‚
    request = HistoricalDataRequest(
        symbol="600519.SH",
        start_date="2024-01-01",
        end_date="2024-01-31",
        period=SupportedPeriod.DAILY,
        include_quality_metrics=True,
        use_cache=True
    )
    
    # è·å–æ•°æ®
    response = await api.get_historical_data(request)
    
    if response.success:
        print(f"è·å–åˆ° {len(response.data)} æ¡æ•°æ®")
        print(f"è´¨é‡è¯„åˆ†: {response.quality_report.get('overall_score', 'N/A')}")
    else:
        print(f"è·å–å¤±è´¥: {response.metadata.get('error')}")

# è¿è¡Œç¤ºä¾‹
asyncio.run(basic_example())
```

## å¤šå‘¨æœŸæ•°æ®è·å–

### æ”¯æŒçš„å‘¨æœŸç±»å‹

| å‘¨æœŸ | æšä¸¾å€¼ | æè¿° | é€‚ç”¨åœºæ™¯ |
|------|--------|------|----------|
| 1m | `SupportedPeriod.MINUTE_1` | 1åˆ†é’Ÿ | é«˜é¢‘äº¤æ˜“ã€æ—¥å†…åˆ†æ |
| 5m | `SupportedPeriod.MINUTE_5` | 5åˆ†é’Ÿ | çŸ­æœŸæŠ€æœ¯åˆ†æ |
| 15m | `SupportedPeriod.MINUTE_15` | 15åˆ†é’Ÿ | æ—¥å†…è¶‹åŠ¿åˆ†æ |
| 30m | `SupportedPeriod.MINUTE_30` | 30åˆ†é’Ÿ | ä¸­çŸ­æœŸåˆ†æ |
| 1h | `SupportedPeriod.HOURLY` | 1å°æ—¶ | æ—¥å†…é•¿æœŸè¶‹åŠ¿ |
| 1d | `SupportedPeriod.DAILY` | æ—¥çº¿ | ä¸­é•¿æœŸåˆ†æ |
| 1w | `SupportedPeriod.WEEKLY` | å‘¨çº¿ | é•¿æœŸè¶‹åŠ¿åˆ†æ |
| 1M | `SupportedPeriod.MONTHLY` | æœˆçº¿ | è¶…é•¿æœŸåˆ†æ |

### å¤šå‘¨æœŸå¹¶å‘è·å–ç¤ºä¾‹

```python
async def multi_period_example():
    api = EnhancedHistoricalDataAPI()
    symbol = "600519.SH"
    start_date = "2024-01-01"
    end_date = "2024-01-05"
    
    # å®šä¹‰å¤šä¸ªå‘¨æœŸ
    periods = [
        SupportedPeriod.DAILY,
        SupportedPeriod.HOURLY,
        SupportedPeriod.MINUTE_15
    ]
    
    # åˆ›å»ºå¹¶å‘ä»»åŠ¡
    tasks = []
    for period in periods:
        request = HistoricalDataRequest(
            symbol=symbol,
            start_date=start_date,
            end_date=end_date,
            period=period,
            use_cache=True
        )
        tasks.append(api.get_historical_data(request))
    
    # å¹¶å‘æ‰§è¡Œ
    responses = await asyncio.gather(*tasks)
    
    # å¤„ç†ç»“æœ
    for period, response in zip(periods, responses):
        if response.success:
            print(f"{period.value}: {len(response.data)} æ¡æ•°æ®")
        else:
            print(f"{period.value}: è·å–å¤±è´¥")
```

## æ•°æ®è´¨é‡ç›‘æ§

### è´¨é‡æŒ‡æ ‡è¯´æ˜

å¢å¼ºAPIæä¾›ä»¥ä¸‹è´¨é‡æŒ‡æ ‡ï¼š

- **å®Œæ•´æ€§è¯„åˆ†** (`completeness_rate`): æ•°æ®å®Œæ•´ç¨‹åº¦ï¼ŒèŒƒå›´0-1
- **å‡†ç¡®æ€§è¯„åˆ†** (`accuracy_score`): æ•°æ®å‡†ç¡®ç¨‹åº¦ï¼ŒèŒƒå›´0-1
- **ä¸€è‡´æ€§è¯„åˆ†** (`consistency_score`): æ•°æ®ä¸€è‡´æ€§ç¨‹åº¦ï¼ŒèŒƒå›´0-1
- **åŠæ—¶æ€§è¯„åˆ†** (`timeliness_score`): æ•°æ®åŠæ—¶æ€§ç¨‹åº¦ï¼ŒèŒƒå›´0-1
- **å¼‚å¸¸æ•°æ®æ•°é‡** (`anomaly_count`): æ£€æµ‹åˆ°çš„å¼‚å¸¸æ•°æ®æ¡æ•°

### è´¨é‡ç›‘æ§ç¤ºä¾‹

```python
async def quality_monitoring_example():
    api = EnhancedHistoricalDataAPI()
    
    request = HistoricalDataRequest(
        symbol="600519.SH",
        start_date="2024-01-01",
        end_date="2024-01-31",
        period=SupportedPeriod.DAILY,
        include_quality_metrics=True,  # å¯ç”¨è´¨é‡ç›‘æ§
        normalize_data=True
    )
    
    response = await api.get_historical_data(request)
    
    if response.success and response.quality_report:
        quality = response.quality_report
        
        print("ğŸ“Š æ•°æ®è´¨é‡æŠ¥å‘Š:")
        print(f"  å®Œæ•´æ€§: {quality.get('completeness_rate', 0):.2%}")
        print(f"  å‡†ç¡®æ€§: {quality.get('accuracy_score', 0):.2f}")
        print(f"  ä¸€è‡´æ€§: {quality.get('consistency_score', 0):.2f}")
        print(f"  å¼‚å¸¸æ•°æ®: {quality.get('anomaly_count', 0)}æ¡")
        
        # è®¡ç®—ç»¼åˆè´¨é‡ç­‰çº§
        avg_score = (
            quality.get('completeness_rate', 0) +
            quality.get('accuracy_score', 0) +
            quality.get('consistency_score', 0)
        ) / 3
        
        if avg_score >= 0.9:
            print("  è´¨é‡ç­‰çº§: ä¼˜ç§€ â­â­â­")
        elif avg_score >= 0.8:
            print("  è´¨é‡ç­‰çº§: è‰¯å¥½ â­â­")
        else:
            print("  è´¨é‡ç­‰çº§: ä¸€èˆ¬ â­")
```

## ç¼“å­˜ä¼˜åŒ–ç­–ç•¥

### ç¼“å­˜TTLç­–ç•¥

å¢å¼ºAPIæ ¹æ®æ•°æ®å‘¨æœŸè‡ªåŠ¨è®¾ç½®ç¼“å­˜TTLï¼š

| æ•°æ®å‘¨æœŸ | ç¼“å­˜TTL | è¯´æ˜ |
|----------|---------|------|
| 1m, 5m | 1å°æ—¶ | çŸ­å‘¨æœŸæ•°æ®å˜åŒ–å¿« |
| 15m, 30m | 2-4å°æ—¶ | ä¸­çŸ­å‘¨æœŸæ•°æ® |
| 1h | 8å°æ—¶ | å°æ—¶çº§æ•°æ® |
| 1d | 24å°æ—¶ | æ—¥çº¿æ•°æ® |
| 1w, 1M | 7å¤© | é•¿å‘¨æœŸæ•°æ®å˜åŒ–æ…¢ |

### ç¼“å­˜æ€§èƒ½æµ‹è¯•

```python
async def cache_performance_test():
    api = EnhancedHistoricalDataAPI()
    
    request = HistoricalDataRequest(
        symbol="600519.SH",
        start_date="2024-01-01",
        end_date="2024-01-05",
        period=SupportedPeriod.DAILY,
        use_cache=True
    )
    
    # ç¬¬ä¸€æ¬¡è¯·æ±‚ï¼ˆå†·ç¼“å­˜ï¼‰
    start_time = time.time()
    response1 = await api.get_historical_data(request)
    first_time = (time.time() - start_time) * 1000
    
    # ç¬¬äºŒæ¬¡è¯·æ±‚ï¼ˆçƒ­ç¼“å­˜ï¼‰
    start_time = time.time()
    response2 = await api.get_historical_data(request)
    second_time = (time.time() - start_time) * 1000
    
    if response1.success and response2.success:
        improvement = (first_time - second_time) / first_time * 100
        print(f"ç¼“å­˜æ€§èƒ½æå‡: {improvement:.1f}%")
        print(f"ç¬¬ä¸€æ¬¡è¯·æ±‚: {first_time:.0f}ms (ç¼“å­˜å‘½ä¸­: {response1.metadata.get('cache_hit', False)})")
        print(f"ç¬¬äºŒæ¬¡è¯·æ±‚: {second_time:.0f}ms (ç¼“å­˜å‘½ä¸­: {response2.metadata.get('cache_hit', False)})")
```

## æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. æ‰¹é‡å¹¶å‘è·å–

```python
async def batch_concurrent_example():
    api = EnhancedHistoricalDataAPI()
    symbols = ["600519.SH", "000001.SZ", "600036.SH"]
    
    # åˆ›å»ºå¹¶å‘ä»»åŠ¡
    tasks = []
    for symbol in symbols:
        request = HistoricalDataRequest(
            symbol=symbol,
            start_date="2024-01-01",
            end_date="2024-01-05",
            period=SupportedPeriod.DAILY,
            use_cache=True
        )
        tasks.append(api.get_historical_data(request))
    
    # å¹¶å‘æ‰§è¡Œ
    start_time = time.time()
    responses = await asyncio.gather(*tasks, return_exceptions=True)
    total_time = (time.time() - start_time) * 1000
    
    print(f"æ‰¹é‡è·å–{len(symbols)}åªè‚¡ç¥¨è€—æ—¶: {total_time:.0f}ms")
    print(f"å¹³å‡æ¯è‚¡ç¥¨: {total_time/len(symbols):.0f}ms")
```

### 2. å†…å­˜ä¼˜åŒ–

```python
import gc

async def memory_optimization_example():
    api = EnhancedHistoricalDataAPI()
    
    # å¤§æ•°æ®é‡è·å–
    request = HistoricalDataRequest(
        symbol="600519.SH",
        start_date="2024-01-01",
        end_date="2024-03-31",
        period=SupportedPeriod.DAILY,
        use_cache=False
    )
    
    response = await api.get_historical_data(request)
    
    if response.success:
        print(f"è·å–åˆ° {len(response.data)} æ¡æ•°æ®")
        
        # å¤„ç†æ•°æ®ååŠæ—¶æ¸…ç†
        # ... æ•°æ®å¤„ç†é€»è¾‘ ...
        
        # æ‰‹åŠ¨åƒåœ¾å›æ”¶
        del response
        gc.collect()
        print("å†…å­˜æ¸…ç†å®Œæˆ")
```

### 3. åˆ†é¡µè·å–å¤§æ•°æ®

```python
async def paginated_data_example():
    api = EnhancedHistoricalDataAPI()
    symbol = "600519.SH"
    
    # åˆ†æ®µè·å–å¤§æ—¶é—´èŒƒå›´çš„æ•°æ®
    start_date = datetime(2023, 1, 1)
    end_date = datetime(2024, 1, 1)
    
    all_data = []
    current_date = start_date
    
    while current_date < end_date:
        segment_end = min(current_date + timedelta(days=30), end_date)
        
        request = HistoricalDataRequest(
            symbol=symbol,
            start_date=current_date.strftime("%Y-%m-%d"),
            end_date=segment_end.strftime("%Y-%m-%d"),
            period=SupportedPeriod.DAILY,
            use_cache=True
        )
        
        response = await api.get_historical_data(request)
        
        if response.success:
            all_data.extend(response.data)
            print(f"è·å– {current_date.strftime('%Y-%m-%d')} åˆ° {segment_end.strftime('%Y-%m-%d')} æ•°æ®: {len(response.data)} æ¡")
        
        current_date = segment_end + timedelta(days=1)
    
    print(f"æ€»å…±è·å– {len(all_data)} æ¡æ•°æ®")
```

## é”™è¯¯å¤„ç†æœºåˆ¶

### é”™è¯¯ç±»å‹

å¢å¼ºAPIå®šä¹‰äº†ä»¥ä¸‹é”™è¯¯ç±»å‹ï¼š

- `DataSourceError`: æ•°æ®æºè¿æ¥æˆ–è®¿é—®é”™è¯¯
- `DataValidationError`: æ•°æ®éªŒè¯é”™è¯¯
- `CacheError`: ç¼“å­˜æ“ä½œé”™è¯¯
- `PeriodConversionError`: å‘¨æœŸè½¬æ¢é”™è¯¯

### é”™è¯¯å¤„ç†ç¤ºä¾‹

```python
async def error_handling_example():
    api = EnhancedHistoricalDataAPI()
    
    # æµ‹è¯•æ— æ•ˆè‚¡ç¥¨ä»£ç 
    request = HistoricalDataRequest(
        symbol="INVALID.XX",
        start_date="2024-01-01",
        end_date="2024-01-05",
        period=SupportedPeriod.DAILY
    )
    
    try:
        response = await api.get_historical_data(request)
        
        if response.success:
            print("æ„å¤–æˆåŠŸ")
        else:
            error_msg = response.metadata.get('error', 'æœªçŸ¥é”™è¯¯')
            print(f"é¢„æœŸå¤±è´¥: {error_msg}")
            
            # æ£€æŸ¥é”™è¯¯è¯¦æƒ…
            if 'error_details' in response.metadata:
                details = response.metadata['error_details']
                print(f"é”™è¯¯ç±»å‹: {details.get('error_type')}")
                print(f"é‡è¯•æ¬¡æ•°: {details.get('retry_count', 0)}")
                
    except Exception as e:
        print(f"å¼‚å¸¸å¤„ç†: {str(e)}")
```

## æœ€ä½³å®è·µ

### 1. å‘¨æœŸé€‰æ‹©ç­–ç•¥

```python
# æ ¹æ®åˆ†æéœ€æ±‚é€‰æ‹©åˆé€‚çš„å‘¨æœŸ
analysis_scenarios = {
    "é«˜é¢‘äº¤æ˜“": SupportedPeriod.MINUTE_1,
    "æ—¥å†…çŸ­çº¿": SupportedPeriod.MINUTE_5,
    "æ—¥å†…ä¸­çº¿": SupportedPeriod.MINUTE_15,
    "æ—¥å†…é•¿çº¿": SupportedPeriod.HOURLY,
    "çŸ­æœŸåˆ†æ": SupportedPeriod.DAILY,
    "ä¸­æœŸåˆ†æ": SupportedPeriod.WEEKLY,
    "é•¿æœŸåˆ†æ": SupportedPeriod.MONTHLY
}
```

### 2. ç¼“å­˜ä½¿ç”¨å»ºè®®

```python
# ç¼“å­˜ä½¿ç”¨å†³ç­–
def should_use_cache(query_frequency: str, data_freshness_requirement: str) -> bool:
    """
    å†³å®šæ˜¯å¦ä½¿ç”¨ç¼“å­˜
    
    Args:
        query_frequency: æŸ¥è¯¢é¢‘ç‡ ("high", "medium", "low")
        data_freshness_requirement: æ•°æ®æ–°é²œåº¦è¦æ±‚ ("real_time", "near_real_time", "historical")
    
    Returns:
        bool: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
    """
    if data_freshness_requirement == "real_time":
        return False
    elif query_frequency == "high":
        return True
    else:
        return data_freshness_requirement == "historical"
```

### 3. è´¨é‡ç›‘æ§é…ç½®

```python
# è´¨é‡ç›‘æ§é…ç½®
quality_thresholds = {
    "completeness_rate": 0.95,  # å®Œæ•´æ€§é˜ˆå€¼
    "accuracy_score": 0.90,     # å‡†ç¡®æ€§é˜ˆå€¼
    "consistency_score": 0.90,  # ä¸€è‡´æ€§é˜ˆå€¼
    "max_anomaly_rate": 0.05    # æœ€å¤§å¼‚å¸¸ç‡
}

def check_data_quality(quality_report: dict) -> bool:
    """æ£€æŸ¥æ•°æ®è´¨é‡æ˜¯å¦è¾¾æ ‡"""
    for metric, threshold in quality_thresholds.items():
        if metric == "max_anomaly_rate":
            anomaly_rate = quality_report.get('anomaly_count', 0) / quality_report.get('total_records', 1)
            if anomaly_rate > threshold:
                return False
        else:
            if quality_report.get(metric, 0) < threshold:
                return False
    return True
```

### 4. æ€§èƒ½ç›‘æ§

```python
import time
from collections import defaultdict

class PerformanceMonitor:
    def __init__(self):
        self.metrics = defaultdict(list)
    
    async def monitored_request(self, api, request):
        """ç›‘æ§APIè¯·æ±‚æ€§èƒ½"""
        start_time = time.time()
        
        try:
            response = await api.get_historical_data(request)
            request_time = (time.time() - start_time) * 1000
            
            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            self.metrics['response_times'].append(request_time)
            self.metrics['success_count'] += 1 if response.success else 0
            self.metrics['cache_hits'] += 1 if response.metadata.get('cache_hit') else 0
            self.metrics['total_requests'] += 1
            
            return response
            
        except Exception as e:
            self.metrics['error_count'] += 1
            raise e
    
    def get_performance_report(self):
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        if not self.metrics['response_times']:
            return "æš‚æ— æ€§èƒ½æ•°æ®"
        
        avg_response_time = sum(self.metrics['response_times']) / len(self.metrics['response_times'])
        success_rate = self.metrics['success_count'] / self.metrics['total_requests']
        cache_hit_rate = self.metrics['cache_hits'] / self.metrics['total_requests']
        
        return {
            'avg_response_time_ms': avg_response_time,
            'success_rate': success_rate,
            'cache_hit_rate': cache_hit_rate,
            'total_requests': self.metrics['total_requests']
        }
```

## ç¤ºä¾‹ä»£ç 

### å®Œæ•´çš„ç”Ÿäº§ç¯å¢ƒç¤ºä¾‹

```python
import asyncio
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ProductionHistoricalDataService:
    """ç”Ÿäº§ç¯å¢ƒå†å²æ•°æ®æœåŠ¡"""
    
    def __init__(self):
        self.api = EnhancedHistoricalDataAPI()
        self.performance_monitor = PerformanceMonitor()
        self.quality_thresholds = {
            "completeness_rate": 0.95,
            "accuracy_score": 0.90,
            "consistency_score": 0.90
        }
    
    async def get_reliable_historical_data(self, 
                                         symbol: str,
                                         start_date: str,
                                         end_date: str,
                                         period: SupportedPeriod,
                                         max_retries: int = 3) -> Dict[str, Any]:
        """
        è·å–å¯é çš„å†å²æ•°æ®
        
        åŒ…å«é‡è¯•æœºåˆ¶ã€è´¨é‡æ£€æŸ¥å’Œæ€§èƒ½ç›‘æ§
        """
        
        for attempt in range(max_retries):
            try:
                request = HistoricalDataRequest(
                    symbol=symbol,
                    start_date=start_date,
                    end_date=end_date,
                    period=period,
                    include_quality_metrics=True,
                    use_cache=True
                )
                
                # ç›‘æ§è¯·æ±‚æ€§èƒ½
                response = await self.performance_monitor.monitored_request(
                    self.api, request
                )
                
                if not response.success:
                    logger.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {response.metadata.get('error')}")
                    continue
                
                # æ£€æŸ¥æ•°æ®è´¨é‡
                if response.quality_report:
                    quality_ok = self.check_data_quality(response.quality_report)
                    if not quality_ok:
                        logger.warning(f"æ•°æ®è´¨é‡ä¸è¾¾æ ‡ (å°è¯• {attempt + 1}/{max_retries})")
                        continue
                
                # æˆåŠŸè¿”å›
                logger.info(f"æˆåŠŸè·å– {symbol} æ•°æ®: {len(response.data)} æ¡")
                return {
                    'success': True,
                    'data': response.data,
                    'quality_report': response.quality_report,
                    'metadata': response.metadata
                }
                
            except Exception as e:
                logger.error(f"è¯·æ±‚å¼‚å¸¸ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
                if attempt == max_retries - 1:
                    raise e
                
                # æŒ‡æ•°é€€é¿
                await asyncio.sleep(2 ** attempt)
        
        return {
            'success': False,
            'error': f'åœ¨ {max_retries} æ¬¡å°è¯•åä»ç„¶å¤±è´¥'
        }
    
    def check_data_quality(self, quality_report: Dict[str, Any]) -> bool:
        """æ£€æŸ¥æ•°æ®è´¨é‡"""
        for metric, threshold in self.quality_thresholds.items():
            if quality_report.get(metric, 0) < threshold:
                logger.warning(f"è´¨é‡æŒ‡æ ‡ {metric} ä¸è¾¾æ ‡: {quality_report.get(metric, 0)} < {threshold}")
                return False
        return True
    
    async def batch_get_historical_data(self, 
                                      symbols: List[str],
                                      start_date: str,
                                      end_date: str,
                                      period: SupportedPeriod,
                                      max_concurrent: int = 5) -> Dict[str, Any]:
        """
        æ‰¹é‡è·å–å†å²æ•°æ®
        
        æ§åˆ¶å¹¶å‘æ•°é‡ï¼Œé¿å…è¿‡è½½
        """
        
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def get_single_symbol_data(symbol: str):
            async with semaphore:
                return await self.get_reliable_historical_data(
                    symbol, start_date, end_date, period
                )
        
        # å¹¶å‘è·å–æ‰€æœ‰è‚¡ç¥¨æ•°æ®
        tasks = [get_single_symbol_data(symbol) for symbol in symbols]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æ•´ç†ç»“æœ
        successful_results = {}
        failed_results = {}
        
        for symbol, result in zip(symbols, results):
            if isinstance(result, Exception):
                failed_results[symbol] = str(result)
            elif result.get('success'):
                successful_results[symbol] = result
            else:
                failed_results[symbol] = result.get('error', 'æœªçŸ¥é”™è¯¯')
        
        return {
            'successful': successful_results,
            'failed': failed_results,
            'success_rate': len(successful_results) / len(symbols),
            'performance_report': self.performance_monitor.get_performance_report()
        }

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    service = ProductionHistoricalDataService()
    
    # å•ä¸ªè‚¡ç¥¨æ•°æ®è·å–
    result = await service.get_reliable_historical_data(
        symbol="600519.SH",
        start_date="2024-01-01",
        end_date="2024-01-31",
        period=SupportedPeriod.DAILY
    )
    
    if result['success']:
        print(f"æˆåŠŸè·å–æ•°æ®: {len(result['data'])} æ¡")
    else:
        print(f"è·å–å¤±è´¥: {result['error']}")
    
    # æ‰¹é‡è‚¡ç¥¨æ•°æ®è·å–
    symbols = ["600519.SH", "000001.SZ", "600036.SH"]
    batch_result = await service.batch_get_historical_data(
        symbols=symbols,
        start_date="2024-01-01",
        end_date="2024-01-05",
        period=SupportedPeriod.DAILY
    )
    
    print(f"æ‰¹é‡è·å–æˆåŠŸç‡: {batch_result['success_rate']:.1%}")
    print(f"æˆåŠŸ: {len(batch_result['successful'])} ä¸ª")
    print(f"å¤±è´¥: {len(batch_result['failed'])} ä¸ª")

if __name__ == "__main__":
    asyncio.run(main())
```

## æ€»ç»“

å¢å¼ºçš„å†å²Kçº¿APIæä¾›äº†å¼ºå¤§çš„åŠŸèƒ½å’Œçµæ´»çš„é…ç½®é€‰é¡¹ï¼Œé€šè¿‡åˆç†ä½¿ç”¨è¿™äº›åŠŸèƒ½ï¼Œå¯ä»¥æ„å»ºé«˜æ€§èƒ½ã€é«˜å¯é æ€§çš„é‡‘èæ•°æ®åº”ç”¨ã€‚

### å…³é”®è¦ç‚¹

1. **é€‰æ‹©åˆé€‚çš„æ•°æ®å‘¨æœŸ**ï¼šæ ¹æ®åˆ†æéœ€æ±‚é€‰æ‹©æœ€é€‚åˆçš„æ—¶é—´å‘¨æœŸ
2. **å¯ç”¨è´¨é‡ç›‘æ§**ï¼šåœ¨ç”Ÿäº§ç¯å¢ƒä¸­å§‹ç»ˆå¯ç”¨æ•°æ®è´¨é‡ç›‘æ§
3. **åˆç†ä½¿ç”¨ç¼“å­˜**ï¼šæ ¹æ®æ•°æ®æ–°é²œåº¦è¦æ±‚å’ŒæŸ¥è¯¢é¢‘ç‡å†³å®šç¼“å­˜ç­–ç•¥
4. **å®ç°é”™è¯¯å¤„ç†**ï¼šåŒ…å«é‡è¯•æœºåˆ¶å’Œé™çº§æ–¹æ¡ˆ
5. **ç›‘æ§æ€§èƒ½æŒ‡æ ‡**ï¼šå®šæœŸç›‘æ§APIæ€§èƒ½å’Œæ•°æ®è´¨é‡
6. **ä¼˜åŒ–å¹¶å‘å¤„ç†**ï¼šä½¿ç”¨å¼‚æ­¥ç¼–ç¨‹å’Œåˆç†çš„å¹¶å‘æ§åˆ¶

é€šè¿‡éµå¾ªè¿™äº›æœ€ä½³å®è·µï¼Œæ‚¨å¯ä»¥å……åˆ†å‘æŒ¥å¢å¼ºAPIçš„ä¼˜åŠ¿ï¼Œæ„å»ºç¨³å®šå¯é çš„é‡‘èæ•°æ®æœåŠ¡ã€‚